dataset_args:
  dataset_path: 'hojjat-m/FarsInstruct'
  streaming: false

model_args:
  model_path: 'bolbolzaban/gpt2-persian'
  vocab_size: 25000

training_args:
  seed: 32
  pin_memory: true
  buffer_size: 10000
  max_len: 256
  output_dir: './results'
  # evaluation_strategy: "steps"
  num_train_epochs: 1 
  max_steps: 35000
  # eval_steps: 1
  save_steps: 200
  logging_steps: 200
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 1
  per_device_eval_batch_size: 1
  learning_rate: 0.001
  optim: "paged_adamw_8bit"
  lr_scheduler_type: 'linear'
  warmup_steps: 100
  gradient_checkpointing: false
  fp16: true
  weight_decay: 0.001
  run_name: "farsinstruct"
  # report_to: "wandb"

quantization_args:
    load_in_4bit: true
    double_quant: true
    quant_type: "nf4"
    lora_rank: 16 
    lora_alpha: 32 
    lora_dropout: 0.05 
