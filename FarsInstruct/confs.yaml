######### PATHS
# llama: /home/hojjat/workstation/FarsInstruct/FarsInstruct/checkpoints/Llama2-7b-hf-raw
# hooshvare: HooshvareLab/gpt2-fa


dataset_args:
    dataset_path: 'hojjat-m/FarsInstruct'
    streaming: false

model_args:
    #tokenizer_path: '/home/hojjat/workstation/FarsInstruct/FarsInstruct/checkpoints/Llama2-7b-hf-raw'
    #model_path: '/home/hojjat/workstation/FarsInstruct/FarsInstruct/checkpoints/Llama2-7b-hf-raw'
    # peft_model: '/home/hojjat/workstation/FarsInstruct/FarsInstruct/results/llama2.snapp-digi-pn_sum-syntran-qa-pharaphrase-reading_comp-parsi_sent/checkpoint-3200'

    # tokenizer_path: '/home/hojjat/workstation/FarsInstruct/FarsInstruct/results/llama2.snapp-digi-pn_sum-syntran-qa-pharaphrase-reading_comp-parsi_sent/checkpoint-3200'
    # model_path: '/home/hojjat/workstation/FarsInstruct/FarsInstruct/results/llama2.snapp-digi-pn_sum-syntran-qa-pharaphrase-reading_comp-parsi_sent/checkpoint-3200'

    # tokenizer_path: HooshvareLab/gpt2-fa
    # model_path: HooshvareLab/gpt2-fa

    tokenizer_path: /media/abbas/Backup/PersianMind-v1.0
    model_path: /media/abbas/Backup/PersianMind-v1.0

    #tokenizer_path: '/media/abbas/Backup/mGPT-1.3B-persian/'
    #model_path: '/media/abbas/Backup/mGPT-1.3B-persian/'

    #tokenizer_path : '/media/abbas/Backup/Mistral-7B-v0.1'
    #model_path : '/media/abbas/Backup/Mistral-7B-Instruct-v0.2'

    #tokenizer_path : '/media/abbas/Backup/Maral-7B-alpha-1'
    #model_path : '/media/abbas/Backup/Maral-7B-alpha-1'

    #vocab_size: 25000
    vocab_size: 42001

training_args:
    run_name: persianmind.train-on-col-2-3.eval-on-col-1-shot-mix
    datasets: persiannlp/parsinlu_sentiment,PNLPhub/digikala-sentiment-analysis,pn_summary,wiki_summary,PNLPhub/DigiMag,PNLPhub/Persian-News,SLPL/syntran-fa,SajjadAyoubi/persian_qa,parsinlu_reading_comprehension,persiannlp/parsinlu_translation_fa_en,persiannlp/parsinlu_translation_en_fa,PNLPhub/PEYMA,PNLPhub/parsinlu-multiple-choice,PNLPhub/C-ExaPPC,PNLPhub/Pars-ABSA,sciq,trivia_qa_unfiltered,persian_ner,p3_xlwic,adversarial_qa_droberta,adversarial_qa_dbert,adversarial_qa_dbidaf
    #datasets: persiannlp/parsinlu_sentiment
    instruction_template: llama
    shots: 1
    seed: 32
    pin_memory: true
    buffer_size: 10000
    max_len: 512
    output_dir: ./results/persianmind.train-on-col-2-3.eval-on-col-1-shot-mix
    # evaluation_strategy: "steps"
    num_train_epochs: 1
    max_steps: -1
    save_steps: 500
    logging_steps: 500
    logging_dir: /home/hojjat/workstation/tensorboardlog
    per_device_train_batch_size: 8
    gradient_accumulation_steps: 2
    learning_rate: 0.001
    optim: paged_adamw_8bit
    lr_scheduler_type: linear
    warmup_steps: 600
    gradient_checkpointing: false
    bf16: true
    fp16: false
    weight_decay: 0.001
    report_to: tensorboard


evaluation_args:

    #model_path: /home/hojjat/workstation/FarsInstruct/FarsInstruct/checkpoints/Llama2-7b-hf-raw
    #tokenizer_path: /home/hojjat/workstation/FarsInstruct/FarsInstruct/checkpoints/Llama2-7b-hf-raw
    #peft_model_id: ./results/llama2.2.snapp-digi-pn_sum-syntran-qa-pharaphrase-reading_comp-parsi_sent/checkpoint-6200
   
    #tokenizer_path: /media/abbas/Backup/PersianMind-v1.0
    #model_path: /media/abbas/Backup/PersianMind-v1.0
    #peft_model_id: ./results/persian_mind.snapp-digi-pn_sum-syntran-qa-pharaphrase-reading_comp-parsi_sent/checkpoint-6200
 
    #tokenizer_path : '/media/abbas/Backup/PersianMind-v1.0'
    #model_path : '/media/abbas/Backup/Mistral-7B-Instruct-v0.2'
    

    # tokenizer_path: HooshvareLab/gpt2-fa
    # model_path: HooshvareLab/gpt2-fa
    # peft_model_id: null
    
    tokenizer_path: null
    model_path: null
    peft_model_id: null

    #datasets: PNLPhub/FarsTail,PNLPhub/digikala-sentiment-analysis,PNLPhub/snappfood-sentiment-analysis,persiannlp/parsinlu_entailment
    datasets: PNLPhub/FarsTail,persiannlp/parsinlu_entailment,persiannlp/parsinlu_query_paraphrasing,PNLPhub/snappfood-sentiment-analysis
    #datasets: persiannlp/parsinlu_entailment
    instruction_template: llama
    shots: 1
    task_type: multiple_choice # 'generate_until'
    batch_size: 8
    max_len: 512

quantization_args:
    load_in_4bit: true
    double_quant: true
    quant_type: "nf4"
    lora_rank: 16
    lora_alpha: 32 
    lora_dropout: 0.05 
