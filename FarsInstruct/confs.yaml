dataset_args:
    dataset_path: 'hojjat-m/FarsInstruct'
    streaming: false

model_args:
    #model_path: 'bolbolzaban/gpt2-persian'
    #model_path: './checkpoints/hooshvare.zs-fs.pn_sum-syntran.-1.bs8'
    #model_path: HooshvareLab/gpt2-fa
    #model_path: /home/hojjat/workstation/FarsInstruct/FarsInstruct/checkpoints/Llama2-7b-hf-raw
    model_path: /media/abbas/Backup/Llama-2-13b-hf
    #model_path: meta-llama/Llama-2-7b-chat-hf
    #model_path: meta-llama/Llama-2-7b
    #model_path: meta-llama/Llama-2-13b
    #model_path: meta-llama/Llama-2-7b-chat-hf
    #model_path: meta-llama/Llama-2-13b-hf
    #model_path: meta-llama/Llama-2-7b-hf
    #vocab_size: 25000
    vocab_size: 42001

training_args:
    desc: 'llama2_13b.zs-fs.digi'
    seed: 32
    pin_memory: true
    buffer_size: 10000
    max_len: 512
    output_dir: './results'
    # evaluation_strategy: "steps"
    num_train_epochs: 2
    max_steps: -1
    # eval_steps: 1
    save_steps: 400
    logging_steps: 400
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 1
    per_device_eval_batch_size: 1
    learning_rate: 0.001
    optim: "adamw_torch"
    lr_scheduler_type: 'linear'
    warmup_steps: 1000
    gradient_checkpointing: false
    fp16: true
    weight_decay: 0.001
    run_name: "farsinstruct"
    report_to: "wandb"

quantization_args:
    load_in_4bit: true
    double_quant: true
    quant_type: "nf4"
    lora_rank: 4
    lora_alpha: 32 
    lora_dropout: 0.05 
